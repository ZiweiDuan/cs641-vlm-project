{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8ed4dae5c650a9f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prerequisite:\n",
    "## Set Up Google Cloud account (billing, service, credentials)\n",
    "- If you are new to Google Cloud, create an account. \n",
    "- In the Google Cloud console, create a Google Cloud project. \n",
    "- Select IAM & Admin service from within this project in the Google Cloud dashboard, create service account and generate service account key. \n",
    "\n",
    "## [Set up a Python development environment that run on Google Cloud](https://cloud.google.com/python/docs/setup)\n",
    "- Install Python;\n",
    "- Use `venv` to create isolated Python environment;  \n",
    "- Install the Cloud Client Libraries for the Python development environment;\n",
    "- [Install and initialize the gcloud CLI.](https://cloud.google.com/sdk/docs/install-sdk)\n",
    "\n",
    "## [Install Vertex AI SDK for Python](https://cloud.google.com/vertex-ai/docs/python-sdk/use-vertex-ai-python-sdk) \n",
    "- Vertex AI is a comprehensive, managed machine learning platform by Google Cloud that allows ML engineers to develop and maintain ML/AL models.\n",
    "- ```\n",
    "  pip install --upgrade google-cloud-aiplatform\n",
    "  ```\n",
    "- Enable all recommended Vertex AI APIs from Vertex AI dashboard, e.g. Gemini Pro via the Gemini API.     \n",
    "\n",
    "  \n",
    "## [Quickstart with Gemini API with Python](https://ai.google.dev/tutorials/python_quickstart)\n",
    "- You can either run Jupyter notebook on your local editor or run codes in Google Colab. \n",
    "- Getting-started/intro_gemini_python.ipynb [github](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_python.ipynb) | [More Ex1](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts?_gl=1*17is47x*_ga*MTY5MzM3MDc0My4xNjc3NjE4OTE4*_ga_WH2QY8WWF5*MTcxMTc1Mjg3Ni45LjEuMTcxMTc1Mzk1OS4wLjAuMA..&_ga=2.179362852.-1693370743.1677618918) | [Ex2](https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afc1d6cc1f124358"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d10cebe5065846a3"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "import IPython\n",
    "import time\n",
    "import re \n",
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "import pathlib\n",
    "import textwrap\n",
    "import IPython.display\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import Video\n",
    "\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "from vertexai.vision_models import (\n",
    "    Video\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T21:08:05.556742Z",
     "start_time": "2024-03-31T21:08:05.552059Z"
    }
   },
   "id": "56c44becbbcdf788"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the Gemini 1.0 Pro Vision Model\n",
    "`geminii-1.0-pro-vision` is a multimodal model that supports multimodal prompts. You can include text, image(s) and video in your prompt requests and get text or code response. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba3e5c31226cac0b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T06:41:03.306967Z",
     "start_time": "2024-03-31T06:41:03.294995Z"
    }
   },
   "id": "6172b9794db239e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions for handling videos contents \n",
    "* Load video from uri\n",
    "* Display video inline\n",
    "* Generate response based on prompt  \n",
    "* Display response in markdown"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32320afc793e323e"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "def load_video_from_gcs(gcs_uri: str):\n",
    "    '''\n",
    "    Load video from uri, which can be found from Google Cloud Storage. \n",
    "    :param gci: must be Google Clous URI (starting with 'gs://') because `Part.from_uri(uri=\"gs://....)` only takes gc uri. The bucket that stores the file must be in the same Google Cloud project that is sending the request. \n",
    "    :return: Part (vertexai.generative_mdels.Part. A part of a multi-part Content message.)\n",
    "    '''\n",
    "    video_uri = \"gs://vlm-videos/dump_trash.mp4\"\n",
    "    return Part.from_uri(video_uri, mime_type=\"video/mp4\")\n",
    "    \n",
    "\n",
    "def display_video_from_gcs(gcs_uri: str):\n",
    "    # Display video in-line in the Jupyter notebook for interactive purpose.\n",
    "    video_url = \"https://storage.cloud.google.com/\" + gcs_uri.replace(\"gs://\", \"\") + \"?authuser=1\"\n",
    "    return IPython.display.Video(video_url)  # must return b/c the displayhook only display the last \"result\" of the cell.\n",
    "    \n",
    "    \n",
    "def generate_response_from_video(prompt: str, video: Part):\n",
    "    content = [prompt, video]\n",
    "    return multimodal_model.generate_content(content)\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T21:30:49.871906Z",
     "start_time": "2024-03-31T21:30:49.854515Z"
    }
   },
   "id": "f6bd154eabe88794"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and display video in-line"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e254be69d64eeb4"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Video object>",
      "text/html": "<video src=\"https://storage.cloud.google.com/vlm-videos/dump_trash.mp4?authuser=1\" controls  >\n      Your browser does not support the <code>video</code> element.\n    </video>"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcsURI = \"gs://vlm-videos/dump_trash.mp4\" \n",
    "video = load_video_from_gcs(gcsURI)\n",
    "display_video_from_gcs(gcsURI)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T21:18:31.621798Z",
     "start_time": "2024-03-31T21:18:31.616084Z"
    }
   },
   "id": "de6d6a35aacadbf1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Generate text response from a video file \n",
    "### Insight 1: response is not always the same. Here are some examples of responses:\n",
    "* `['A hand came into the scene and threw the tomatoes into the trash one by one',\n",
    " 'Then the hand threw the empty bottle into the trash',\n",
    " 'Finally, the hand balled up the paper towel and threw it into the trash']`\n",
    "\n",
    "* `A person is throwing 2 tomatoes into a trash can.`\n",
    "\n",
    "### Insight 2: prompt engineering by breaking into sub-questions  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33f4105a215ccf1a"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": ">  A person is throwing 2 tomatoes into a trash can."
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "What action was performed? \n",
    "\"\"\"\n",
    "response = generate_response_from_video(prompt, video)\n",
    "to_markdown(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T21:31:25.800770Z",
     "start_time": "2024-03-31T21:31:18.609578Z"
    }
   },
   "id": "bd5a7634b2e24170"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "What action was performed?\n",
    "How many objects are there in the video?\n",
    "Which objects stay stationary in the video?\n",
    "Which objects change their positions in the video? \n",
    "\"\"\"\n",
    "contents = [prompt, video]\n",
    "responses = multimodal_model.generate_content(contents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T21:35:34.839603Z",
     "start_time": "2024-03-31T21:35:27.926044Z"
    }
   },
   "id": "d67ef9772e37ee09"
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": ">  1. A hand enters the scene and picks up a clear plastic bottle.\n> 2. There are 5 objects in the scene: \n>  - a clear plastic bottle\n>  - a can\n>  - 2 tomatoes\n>  - a paper towel\n> 3. The clear plastic bottle, the can, and the paper towel stay stationary.\n> 4. The hand and the tomatoes change their positions."
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(responses.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T21:58:16.243107Z",
     "start_time": "2024-03-31T21:58:16.228127Z"
    }
   },
   "id": "b05bf6261119c8f6"
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "['A hand enters the scene and picks up a clear plastic bottle.',\n 'There are 5 objects in the scene: \\n - a clear plastic bottle\\n - a can\\n - 2 tomatoes\\n - a paper towel',\n 'The clear plastic bottle, the can, and the paper towel stay stationary.',\n 'The hand and the tomatoes change their positions.']"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = [r.strip() for r in re.split(r'\\n?\\d\\.\\s', responses.text) if r.strip()]\n",
    "parts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T21:59:43.980665Z",
     "start_time": "2024-03-31T21:59:43.970367Z"
    }
   },
   "id": "b42b93168245ee44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
